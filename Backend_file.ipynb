{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b73152-3be9-4a69-ada8-bb8d84376e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Paths to datasets\n",
    "data_path_train = 'datasets/train'\n",
    "data_path_test = 'datasets/test'\n",
    "data_path_val = 'datasets/validation'\n",
    "\n",
    "# Validate and convert images\n",
    "def validate_and_convert_images(directory):\n",
    "    supported_formats = ['JPEG', 'PNG', 'GIF', 'BMP']\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    if img.format not in supported_formats:\n",
    "                        print(f\"Unsupported format: {file_path} - {img.format}\")\n",
    "                        # Convert to a supported format\n",
    "                        new_file_path = os.path.splitext(file_path)[0] + '.png'\n",
    "                        img.convert('RGB').save(new_file_path, 'PNG')\n",
    "                        os.remove(file_path)\n",
    "                        print(f\"Converted {file_path} to {new_file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Validate and convert images in all dataset directories\n",
    "validate_and_convert_images(data_path_train)\n",
    "validate_and_convert_images(data_path_val)\n",
    "validate_and_convert_images(data_path_test)\n",
    "\n",
    "# Image dimensions\n",
    "img_width = 180\n",
    "img_height = 180\n",
    "\n",
    "# Load datasets\n",
    "data_train = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_path_train,\n",
    "    shuffle=True,\n",
    "    image_size=(img_width, img_height),\n",
    "    batch_size=32)\n",
    "\n",
    "data_val = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_path_val,\n",
    "    shuffle=False,\n",
    "    image_size=(img_width, img_height),\n",
    "    batch_size=32)\n",
    "\n",
    "data_test = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_path_test,\n",
    "    shuffle=False,\n",
    "    image_size=(img_width, img_height),\n",
    "    batch_size=32)\n",
    "\n",
    "# Class names\n",
    "data_cat = data_train.class_names\n",
    "\n",
    "# Display some images from the training dataset\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in data_train.take(1):\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(data_cat[labels[i]])\n",
    "        plt.axis('off')\n",
    "\n",
    "# Data augmentation layer\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.1),\n",
    "        layers.RandomContrast(0.1),\n",
    "        layers.RandomBrightness(0.1),\n",
    "        layers.RandomTranslation(0.2, 0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Enhanced Model definition with more layers and adjusted regularization\n",
    "model = keras.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(256, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(len(data_cat), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Callback for saving the best model\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.keras',\n",
    "    monitor='val_loss',  # changed from val_accuracy to val_loss\n",
    "    save_best_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "# Callback for early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=25, restore_best_weights=True)  # reduced patience\n",
    "\n",
    "# Callback for reducing learning rate on plateau\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=25, min_lr=0.00001)  # more aggressive reduction\n",
    "\n",
    "# Train the model with early stopping and learning rate reduction\n",
    "history = model.fit(data_train, validation_data=data_val, epochs=120,\n",
    "                    callbacks=[checkpoint, early_stopping, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a365e-6390-459b-8ad5-46c1ce821e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "model.save('image_classification_test_5.keras')\n",
    "\n",
    "# Plot training and validation accuracy and loss\n",
    "epochs_range = range(len(history.history['accuracy']))\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(epochs_range, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, history.history['loss'], label='Training Loss')\n",
    "plt.plot(epochs_range, history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Load the saved model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b149f6-bee5-4655-b0ae-f0248860c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('image_classification_test_2.keras')\n",
    "# Predict on a single image\n",
    "image_path = 'datasets/test/apple/195.jpg'\n",
    "image = tf.keras.utils.load_img(image_path, target_size=(img_height, img_width))\n",
    "img_arr = tf.keras.utils.img_to_array(image)\n",
    "img_bat = tf.expand_dims(img_arr, 0)\n",
    "\n",
    "predict = loaded_model.predict(img_bat)\n",
    "score = tf.nn.softmax(predict[0])\n",
    "\n",
    "print(f'The given image is {data_cat[np.argmax(score)]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c083ada-5117-4b2b-8bcf-401f00dfe350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
